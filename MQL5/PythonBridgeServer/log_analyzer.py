"""
MQL5 Log Analyzer
Analyzes structured logs generated by CMLLogger.

Usage:
    python log_analyzer.py --log MLLogs/MLPoweredEA/20241118.csv

Features:
    - Parse structured CSV logs
    - Generate performance reports
    - Detect anomalies
    - Visualize metrics
"""

import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from loguru import logger


class MQL5LogAnalyzer:
    """Analyzer for MQL5 structured logs."""

    def __init__(self, log_file: Path):
        self.log_file = log_file
        self.df: Optional[pd.DataFrame] = None
        self.model_logs: Optional[pd.DataFrame] = None
        self.trade_logs: Optional[pd.DataFrame] = None

    def load_logs(self) -> bool:
        """Load and parse log file."""
        try:
            self.df = pd.read_csv(self.log_file)

            # Parse timestamps
            self.df["Timestamp"] = pd.to_datetime(self.df["Timestamp"])

            # Separate by log type
            if "Level" in self.df.columns:
                # General logs
                logger.info(f"Loaded {len(self.df)} log entries")

                # Extract model inference logs
                model_mask = self.df["Level"] == "MODEL_INFERENCE"
                if model_mask.any():
                    self.model_logs = self.df[model_mask].copy()
                    logger.info(f"Found {len(self.model_logs)} model inference logs")

                # Extract trade execution logs
                trade_mask = self.df["Level"] == "TRADE_EXECUTION"
                if trade_mask.any():
                    self.trade_logs = self.df[trade_mask].copy()
                    logger.info(f"Found {len(self.trade_logs)} trade execution logs")

            return True

        except Exception as e:
            logger.error(f"Failed to load logs: {e}")
            return False

    def generate_performance_report(self) -> Dict:
        """Generate comprehensive performance report."""
        report = {
            "summary": self._generate_summary(),
            "model_performance": self._analyze_model_performance(),
            "trade_performance": self._analyze_trade_performance(),
            "anomalies": self._detect_anomalies(),
        }

        return report

    def _generate_summary(self) -> Dict:
        """Generate summary statistics."""
        if self.df is None:
            return {}

        summary = {
            "total_entries": len(self.df),
            "time_range": (
                self.df["Timestamp"].min().strftime("%Y-%m-%d %H:%M:%S"),
                self.df["Timestamp"].max().strftime("%Y-%m-%d %H:%M:%S"),
            ),
            "duration_hours": (
                self.df["Timestamp"].max() - self.df["Timestamp"].min()
            ).total_seconds()
            / 3600,
        }

        # Count by level
        if "Level" in self.df.columns:
            level_counts = self.df["Level"].value_counts().to_dict()
            summary["entries_by_level"] = level_counts

        return summary

    def _analyze_model_performance(self) -> Dict:
        """Analyze model inference performance."""
        if self.model_logs is None or len(self.model_logs) == 0:
            return {}

        # Parse model-specific columns
        # Assuming CSV format: Timestamp,Strategy,MagicNumber,Level,PredictionScore,PredictedClass,LatencyUS,ModelVersion

        analysis = {}

        # Latency analysis
        if "LatencyUS" in self.model_logs.columns:
            latencies = pd.to_numeric(self.model_logs["LatencyUS"], errors="coerce")
            latencies = latencies.dropna()

            analysis["latency"] = {
                "mean_us": float(latencies.mean()),
                "median_us": float(latencies.median()),
                "p95_us": float(latencies.quantile(0.95)),
                "p99_us": float(latencies.quantile(0.99)),
                "max_us": float(latencies.max()),
            }

        # Prediction distribution
        if "PredictionScore" in self.model_logs.columns:
            scores = pd.to_numeric(self.model_logs["PredictionScore"], errors="coerce")
            scores = scores.dropna()

            analysis["prediction_distribution"] = {
                "mean": float(scores.mean()),
                "std": float(scores.std()),
                "min": float(scores.min()),
                "max": float(scores.max()),
                "quartiles": {
                    "q25": float(scores.quantile(0.25)),
                    "q50": float(scores.quantile(0.50)),
                    "q75": float(scores.quantile(0.75)),
                },
            }

        # Class distribution
        if "PredictedClass" in self.model_logs.columns:
            class_counts = self.model_logs["PredictedClass"].value_counts().to_dict()
            total = sum(class_counts.values())
            class_pct = {k: v / total * 100 for k, v in class_counts.items()}

            analysis["class_distribution"] = {"counts": class_counts, "percentages": class_pct}

        return analysis

    def _analyze_trade_performance(self) -> Dict:
        """Analyze trade execution performance."""
        if self.trade_logs is None or len(self.trade_logs) == 0:
            return {}

        # Parse trade-specific columns
        # Format: Timestamp,Strategy,MagicNumber,Level,Ticket,SignalType,EntryPrice,SL,TP,PositionSize,Confidence

        analysis = {}

        # Signal distribution
        if "SignalType" in self.trade_logs.columns:
            signal_counts = self.trade_logs["SignalType"].value_counts().to_dict()
            total = sum(signal_counts.values())
            signal_pct = {k: v / total * 100 for k, v in signal_counts.items()}

            analysis["signal_distribution"] = {"counts": signal_counts, "percentages": signal_pct}

        # Confidence analysis
        if "Confidence" in self.trade_logs.columns:
            confidence = pd.to_numeric(self.trade_logs["Confidence"], errors="coerce")
            confidence = confidence.dropna()

            analysis["confidence"] = {
                "mean": float(confidence.mean()),
                "median": float(confidence.median()),
                "min": float(confidence.min()),
                "max": float(confidence.max()),
            }

        # Position sizing
        if "PositionSize" in self.trade_logs.columns:
            sizes = pd.to_numeric(self.trade_logs["PositionSize"], errors="coerce")
            sizes = sizes.dropna()

            analysis["position_size"] = {
                "mean": float(sizes.mean()),
                "median": float(sizes.median()),
                "min": float(sizes.min()),
                "max": float(sizes.max()),
            }

        return analysis

    def _detect_anomalies(self) -> Dict:
        """Detect anomalies in logs."""
        anomalies = {"high_latency_events": [], "extreme_predictions": [], "errors": []}

        # High latency events (> 1 second = 1,000,000 microseconds)
        if self.model_logs is not None and "LatencyUS" in self.model_logs.columns:
            latencies = pd.to_numeric(self.model_logs["LatencyUS"], errors="coerce")
            high_latency_mask = latencies > 1_000_000

            if high_latency_mask.any():
                high_latency_events = self.model_logs[high_latency_mask][
                    ["Timestamp", "LatencyUS"]
                ].to_dict("records")

                anomalies["high_latency_events"] = high_latency_events
                logger.warning(f"Found {len(high_latency_events)} high latency events (>1s)")

        # Extreme predictions (|score| > 0.95)
        if self.model_logs is not None and "PredictionScore" in self.model_logs.columns:
            scores = pd.to_numeric(self.model_logs["PredictionScore"], errors="coerce")
            extreme_mask = scores.abs() > 0.95

            if extreme_mask.any():
                extreme_predictions = self.model_logs[extreme_mask][
                    ["Timestamp", "PredictionScore", "PredictedClass"]
                ].to_dict("records")

                anomalies["extreme_predictions"] = extreme_predictions
                logger.info(f"Found {len(extreme_predictions)} extreme predictions (|score|>0.95)")

        # Error entries
        if self.df is not None and "Level" in self.df.columns:
            error_mask = self.df["Level"].isin(["ERROR", "FATAL"])

            if error_mask.any():
                errors = self.df[error_mask][["Timestamp", "Level", "Message", "Function"]].to_dict(
                    "records"
                )

                anomalies["errors"] = errors
                logger.warning(f"Found {len(errors)} error entries")

        return anomalies

    def plot_latency_over_time(self, save_path: Optional[Path] = None):
        """Plot inference latency over time."""
        if self.model_logs is None or "LatencyUS" not in self.model_logs.columns:
            logger.warning("No latency data to plot")
            return

        plt.figure(figsize=(12, 6))

        # Convert to milliseconds for readability
        latencies_ms = pd.to_numeric(self.model_logs["LatencyUS"], errors="coerce") / 1000
        timestamps = self.model_logs["Timestamp"]

        plt.plot(timestamps, latencies_ms, alpha=0.6, linewidth=1)
        plt.xlabel("Time")
        plt.ylabel("Latency (ms)")
        plt.title("Model Inference Latency Over Time")
        plt.grid(True, alpha=0.3)

        # Add mean line
        mean_latency = latencies_ms.mean()
        plt.axhline(y=mean_latency, color="r", linestyle="--", label=f"Mean: {mean_latency:.2f}ms")

        # Add p95 line
        p95_latency = latencies_ms.quantile(0.95)
        plt.axhline(
            y=p95_latency, color="orange", linestyle="--", label=f"P95: {p95_latency:.2f}ms"
        )

        plt.legend()
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300)
            logger.info(f"Saved latency plot to {save_path}")
        else:
            plt.show()

        plt.close()

    def plot_prediction_distribution(self, save_path: Optional[Path] = None):
        """Plot distribution of prediction scores."""
        if self.model_logs is None or "PredictionScore" not in self.model_logs.columns:
            logger.warning("No prediction data to plot")
            return

        scores = pd.to_numeric(self.model_logs["PredictionScore"], errors="coerce")
        scores = scores.dropna()

        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # Histogram
        axes[0].hist(scores, bins=50, alpha=0.7, edgecolor="black")
        axes[0].axvline(x=0, color="r", linestyle="--", label="Neutral")
        axes[0].set_xlabel("Prediction Score")
        axes[0].set_ylabel("Frequency")
        axes[0].set_title("Distribution of Prediction Scores")
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # Box plot
        axes[1].boxplot(scores, vert=True)
        axes[1].axhline(y=0, color="r", linestyle="--", alpha=0.5)
        axes[1].set_ylabel("Prediction Score")
        axes[1].set_title("Prediction Score Box Plot")
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300)
            logger.info(f"Saved prediction distribution plot to {save_path}")
        else:
            plt.show()

        plt.close()

    def plot_class_distribution(self, save_path: Optional[Path] = None):
        """Plot distribution of predicted classes."""
        if self.model_logs is None or "PredictedClass" not in self.model_logs.columns:
            logger.warning("No class data to plot")
            return

        class_counts = self.model_logs["PredictedClass"].value_counts()

        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # Bar chart
        class_counts.plot(kind="bar", ax=axes[0], alpha=0.7, edgecolor="black")
        axes[0].set_xlabel("Predicted Class")
        axes[0].set_ylabel("Count")
        axes[0].set_title("Predicted Class Distribution")
        axes[0].grid(True, alpha=0.3, axis="y")

        # Pie chart
        axes[1].pie(class_counts, labels=class_counts.index, autopct="%1.1f%%", startangle=90)
        axes[1].set_title("Predicted Class Proportions")

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300)
            logger.info(f"Saved class distribution plot to {save_path}")
        else:
            plt.show()

        plt.close()

    def plot_latency_heatmap(self, save_path: Optional[Path] = None):
        """Plot latency heatmap by hour of day and day of week."""
        if self.model_logs is None or "LatencyUS" not in self.model_logs.columns:
            logger.warning("No latency data to plot")
            return

        df = self.model_logs.copy()
        df["LatencyMS"] = pd.to_numeric(df["LatencyUS"], errors="coerce") / 1000
        df["Hour"] = df["Timestamp"].dt.hour
        df["DayOfWeek"] = df["Timestamp"].dt.day_name()

        # Create pivot table
        pivot = df.pivot_table(
            values="LatencyMS", index="DayOfWeek", columns="Hour", aggfunc="mean"
        )

        # Reorder days
        day_order = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
        pivot = pivot.reindex([day for day in day_order if day in pivot.index])

        plt.figure(figsize=(14, 6))
        sns.heatmap(pivot, annot=True, fmt=".2f", cmap="YlOrRd", cbar_kws={"label": "Latency (ms)"})
        plt.title("Average Inference Latency by Day and Hour")
        plt.xlabel("Hour of Day")
        plt.ylabel("Day of Week")
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300)
            logger.info(f"Saved latency heatmap to {save_path}")
        else:
            plt.show()

        plt.close()

    def export_summary_report(self, output_path: Path):
        """Export comprehensive summary report as HTML."""
        report = self.generate_performance_report()

        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>MQL5 Log Analysis Report</title>
            <style>
                body {{
                    font-family: Arial, sans-serif;
                    margin: 20px;
                    background-color: #f5f5f5;
                }}
                .container {{
                    max-width: 1200px;
                    margin: 0 auto;
                    background-color: white;
                    padding: 30px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }}
                h1 {{
                    color: #2c3e50;
                    border-bottom: 3px solid #3498db;
                    padding-bottom: 10px;
                }}
                h2 {{
                    color: #34495e;
                    margin-top: 30px;
                }}
                .metric {{
                    background-color: #ecf0f1;
                    padding: 15px;
                    margin: 10px 0;
                    border-radius: 5px;
                }}
                .metric-label {{
                    font-weight: bold;
                    color: #7f8c8d;
                }}
                .metric-value {{
                    font-size: 1.2em;
                    color: #2c3e50;
                }}
                .warning {{
                    background-color: #fff3cd;
                    border-left: 4px solid #ffc107;
                    padding: 15px;
                    margin: 10px 0;
                }}
                .error {{
                    background-color: #f8d7da;
                    border-left: 4px solid #dc3545;
                    padding: 15px;
                    margin: 10px 0;
                }}
                table {{
                    width: 100%;
                    border-collapse: collapse;
                    margin: 20px 0;
                }}
                th, td {{
                    padding: 12px;
                    text-align: left;
                    border-bottom: 1px solid #ddd;
                }}
                th {{
                    background-color: #3498db;
                    color: white;
                }}
                tr:hover {{
                    background-color: #f5f5f5;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <h1>MQL5 Log Analysis Report</h1>
                <p><strong>Generated:</strong> {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
                <p><strong>Log File:</strong> {self.log_file}</p>
        """

        # Summary section
        if "summary" in report:
            summary = report["summary"]
            html += f"""
                <h2>Summary</h2>
                <div class="metric">
                    <span class="metric-label">Total Entries:</span>
                    <span class="metric-value">{summary.get('total_entries', 0):,}</span>
                </div>
                <div class="metric">
                    <span class="metric-label">Time Range:</span>
                    <span class="metric-value">{summary.get('time_range', ['N/A', 'N/A'])[0]} to {summary.get('time_range', ['N/A', 'N/A'])[1]}</span>
                </div>
                <div class="metric">
                    <span class="metric-label">Duration:</span>
                    <span class="metric-value">{summary.get('duration_hours', 0):.2f} hours</span>
                </div>
            """

        # Model performance section
        if "model_performance" in report and report["model_performance"]:
            model_perf = report["model_performance"]
            html += "<h2>Model Performance</h2>"

            if "latency" in model_perf:
                latency = model_perf["latency"]
                html += f"""
                <h3>Inference Latency</h3>
                <table>
                    <tr>
                        <th>Metric</th>
                        <th>Value (ms)</th>
                    </tr>
                    <tr>
                        <td>Mean</td>
                        <td>{latency['mean_us']/1000:.3f}</td>
                    </tr>
                    <tr>
                        <td>Median</td>
                        <td>{latency['median_us']/1000:.3f}</td>
                    </tr>
                    <tr>
                        <td>P95</td>
                        <td>{latency['p95_us']/1000:.3f}</td>
                    </tr>
                    <tr>
                        <td>P99</td>
                        <td>{latency['p99_us']/1000:.3f}</td>
                    </tr>
                    <tr>
                        <td>Max</td>
                        <td>{latency['max_us']/1000:.3f}</td>
                    </tr>
                </table>
                """

            if "prediction_distribution" in model_perf:
                pred_dist = model_perf["prediction_distribution"]
                html += f"""
                <h3>Prediction Distribution</h3>
                <table>
                    <tr>
                        <th>Statistic</th>
                        <th>Value</th>
                    </tr>
                    <tr>
                        <td>Mean</td>
                        <td>{pred_dist['mean']:.4f}</td>
                    </tr>
                    <tr>
                        <td>Std Dev</td>
                        <td>{pred_dist['std']:.4f}</td>
                    </tr>
                    <tr>
                        <td>Min</td>
                        <td>{pred_dist['min']:.4f}</td>
                    </tr>
                    <tr>
                        <td>Max</td>
                        <td>{pred_dist['max']:.4f}</td>
                    </tr>
                </table>
                """

            if "class_distribution" in model_perf:
                class_dist = model_perf["class_distribution"]
                html += f"""
                <h3>Class Distribution</h3>
                <table>
                    <tr>
                        <th>Class</th>
                        <th>Count</th>
                        <th>Percentage</th>
                    </tr>
                """
                for cls, count in class_dist["counts"].items():
                    pct = class_dist["percentages"][cls]
                    html += f"""
                    <tr>
                        <td>{cls}</td>
                        <td>{count}</td>
                        <td>{pct:.2f}%</td>
                    </tr>
                    """
                html += "</table>"

        # Anomalies section
        if "anomalies" in report:
            anomalies = report["anomalies"]
            html += "<h2>Anomalies</h2>"

            if anomalies.get("high_latency_events"):
                html += f"""
                <div class="warning">
                    <strong>⚠️ High Latency Events:</strong> 
                    Found {len(anomalies['high_latency_events'])} events with latency > 1 second
                </div>
                """

            if anomalies.get("extreme_predictions"):
                html += f"""
                <div class="warning">
                    <strong>⚠️ Extreme Predictions:</strong> 
                    Found {len(anomalies['extreme_predictions'])} predictions with |score| > 0.95
                </div>
                """

            if anomalies.get("errors"):
                html += f"""
                <div class="error">
                    <strong>❌ Errors:</strong> 
                    Found {len(anomalies['errors'])} error entries
                </div>
                <table>
                    <tr>
                        <th>Timestamp</th>
                        <th>Level</th>
                        <th>Message</th>
                    </tr>
                """
                for error in anomalies["errors"][:10]:  # Show first 10
                    html += f"""
                    <tr>
                        <td>{error.get('Timestamp', 'N/A')}</td>
                        <td>{error.get('Level', 'N/A')}</td>
                        <td>{error.get('Message', 'N/A')}</td>
                    </tr>
                    """
                html += "</table>"

        html += """
            </div>
        </body>
        </html>
        """

        # Write HTML file
        with open(output_path, "w") as f:
            f.write(html)

        logger.info(f"Exported summary report to {output_path}")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Analyze MQL5 structured logs")

    parser.add_argument("--log", type=str, required=True, help="Path to log file (CSV format)")

    parser.add_argument(
        "--output-dir",
        type=str,
        default="analysis_output",
        help="Output directory for reports and plots",
    )

    parser.add_argument("--plots", action="store_true", help="Generate visualization plots")

    args = parser.parse_args()

    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)

    # Create analyzer
    analyzer = MQL5LogAnalyzer(Path(args.log))

    # Load logs
    if not analyzer.load_logs():
        logger.error("Failed to load logs")
        return

    # Generate report
    logger.info("Generating performance report...")
    report = analyzer.generate_performance_report()

    # Print summary to console
    print("\n" + "=" * 70)
    print("MQL5 LOG ANALYSIS SUMMARY")
    print("=" * 70)

    if "summary" in report:
        summary = report["summary"]
        print(f"\nTotal Entries: {summary.get('total_entries', 0):,}")
        print(
            f"Time Range: {summary.get('time_range', ['N/A', 'N/A'])[0]} to {summary.get('time_range', ['N/A', 'N/A'])[1]}"
        )
        print(f"Duration: {summary.get('duration_hours', 0):.2f} hours")

    if "model_performance" in report and report["model_performance"]:
        model_perf = report["model_performance"]
        print("\nModel Performance:")

        if "latency" in model_perf:
            latency = model_perf["latency"]
            print(f"  Avg Latency: {latency['mean_us']/1000:.2f}ms")
            print(f"  P95 Latency: {latency['p95_us']/1000:.2f}ms")

        if "class_distribution" in model_perf:
            class_dist = model_perf["class_distribution"]
            print("\n  Class Distribution:")
            for cls, pct in class_dist["percentages"].items():
                print(f"    {cls}: {pct:.2f}%")

    if "anomalies" in report:
        anomalies = report["anomalies"]
        if any(anomalies.values()):
            print("\nAnomalies Detected:")
            if anomalies.get("high_latency_events"):
                print(f"  ⚠️  High latency events: {len(anomalies['high_latency_events'])}")
            if anomalies.get("extreme_predictions"):
                print(f"  ⚠️  Extreme predictions: {len(anomalies['extreme_predictions'])}")
            if anomalies.get("errors"):
                print(f"  ❌ Errors: {len(anomalies['errors'])}")

    print("=" * 70 + "\n")

    # Export HTML report
    html_path = output_dir / "analysis_report.html"
    analyzer.export_summary_report(html_path)
    print(f"✓ Exported HTML report: {html_path}")

    # Generate plots if requested
    if args.plots:
        logger.info("Generating visualization plots...")

        analyzer.plot_latency_over_time(output_dir / "latency_over_time.png")
        analyzer.plot_prediction_distribution(output_dir / "prediction_distribution.png")
        analyzer.plot_class_distribution(output_dir / "class_distribution.png")
        analyzer.plot_latency_heatmap(output_dir / "latency_heatmap.png")

        print(f"✓ Generated plots in: {output_dir}")


if __name__ == "__main__":
    main()
